// const chatWithLLM = async (prompt) => {
//   try {
//     const response = await fetch("http://localhost:11434/api/generate", {
//       method: "POST",
//       headers: {
//         "Content-Type": "application/json",
//       },
//       body: JSON.stringify({
//         model: "deepseek-r1:8b", // Changed to match your local model
//         prompt: prompt,
//         stream: false,
//       }),
//     })

//     if (!response.ok) {
//       throw new Error(`LLM API error: ${response.status}`)
//     }

//     const data = await response.json()
//     return data.response
//   } catch (error) {
//     console.error("Error calling LLM API:", error)
//     throw error
//   }
// }

// module.exports = { chatWithLLM }
